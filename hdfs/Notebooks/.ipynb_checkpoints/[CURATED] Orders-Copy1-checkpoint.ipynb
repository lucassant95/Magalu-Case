{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('/opt/workspace/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing working directory to root to use custom libs\n",
    "os.chdir('/opt/workspace/')\n",
    "\n",
    "#Spark Configurations\n",
    "    # Sets Session to use spark master container\n",
    "    # Sets Session to use warehouse directory in /opt/workspace/Warehouse and to infer data Schema\n",
    "    # Sets overide mode to dynamic, so we can append data and overwrite old data based on partition\n",
    "spark = SparkSession.builder.appName('[CURATED] Orders') \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", '/opt/workspace/Warehouse') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\", 'dynamic') \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original query I created in case id_pedido was an PK. In this case, I'm building an array collumn\n",
    "# to aggregate all the data from the items of an order in a single row, keeping the id_pedido column as a PK\n",
    "\n",
    "df = spark.sql(\n",
    "    \"\"\"\n",
    "    select \n",
    "        o.order_unique_id,\n",
    "        o.id_pedido,\n",
    "        o.id_parceiro,\n",
    "        o.id_cliente,\n",
    "        o.id_filial,\n",
    "        f.id_cidade,\n",
    "        e.id_estado,\n",
    "        c.id_categoria as categoria,\n",
    "        sc.id_subcategoria as subcategoria,\n",
    "        o.partition_date,\n",
    "        collect_list(map('id_produto', o.id_produto,\n",
    "                        'qtd', o.quantidade,\n",
    "                        'vr_unitario', o.vr_unitario)) as order_partner_items,\n",
    "        round(o.vr_total_pago) as order_partner_value\n",
    "        from curated.normalized_orders o\n",
    "            join raw.produto p on p.id_produto = o.id_produto\n",
    "            join raw.subcategoria sc on sc.id_subcategoria = p.id_subcategoria\n",
    "            join raw.categoria c on c.id_categoria = sc.id_categoria\n",
    "            join raw.filial f on f.id_filial = o.id_filial\n",
    "            join raw.cidade ci on ci.id_cidade = f.id_cidade\n",
    "            join raw.estado e on e.id_estado = ci.id_estado\n",
    "        group by o.order_unique_id, o.id_pedido, o.id_parceiro, o.id_cliente, o.id_filial, f.id_cidade, e.id_estado, o.partition_date,\n",
    "        c.id_categoria, sc.id_subcategoria, vr_total_pago\n",
    "\"\"\")\n",
    "\n",
    "df.write.partitionBy('partition_date').format('parquet').mode('overwrite').saveAsTable('curated.orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from curated.orders limit 20').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stops spark client and finishes the job\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

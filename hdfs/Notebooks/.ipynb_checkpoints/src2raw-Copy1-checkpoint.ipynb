{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import date_format, col, to_date, isnan\n",
    "import os\n",
    "import pyspark\n",
    "import sys\n",
    "from pyspark.sql.utils import AnalysisException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local vars and spark configs\n",
    "ACCESS_KEY = 'AKIAJQ3BUYJREQ6EILHQ'\n",
    "SECRET_KEY = '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'\n",
    "#\"spark://spark-master:7077\" \\\n",
    "#\"local[*]\"\n",
    "#        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "spark = SparkSession.builder.appName('s3_extraction') \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", '/opt/workspace/') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\", 'dynamic') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "        .getOrCreate()\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:2.7.3\" pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files to import and ETL properties\n",
    "files = [\n",
    "    {\n",
    "        'name': 'restaurant.csv.gz',\n",
    "        'format': 'csv',\n",
    "        'partition_col' : 'created_at',\n",
    "        'validation_col': 'id',\n",
    "        'validations': ['nullity', 'duplication']\n",
    "    },\n",
    "    {\n",
    "        'name': 'consumer.csv.gz',\n",
    "        'format': 'csv',\n",
    "        'partition_col' : 'created_at',\n",
    "        'validation_col': 'customer_id',\n",
    "        'validations': ['nullity', 'duplication']\n",
    "    },\n",
    "    {\n",
    "        'name': 'order.json.gz',\n",
    "        'format': 'json',\n",
    "        'partition_col' : 'order_created_at',\n",
    "        'validation_col': 'order_id',\n",
    "        'validations': ['nullity']\n",
    "    },\n",
    "    {\n",
    "        'name': 'status.json.gz',\n",
    "        'format': 'json',\n",
    "        'partition_col' : 'created_at',\n",
    "        'validation_col': 'order_id',\n",
    "        'validations': ['nullity']\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_validation(df, col_to_check, file_name, validations = ['nullity', 'duplication']):\n",
    "    \n",
    "    if 'nullity' in validations:\n",
    "        null_count = df.filter((df[col_to_check] == \"\") | df[col_to_check].isNull() | isnan(df[col_to_check])).count()\n",
    "        assert null_count == 0\n",
    "    \n",
    "    if 'duplication' in validations:\n",
    "        try:\n",
    "            destination_df = spark.sql(f'select {col_to_check} from raw.{file_name.split(\".\")[0]}')\n",
    "            duplicates_count = df.join(destination_df, on=col_to_check).count()\n",
    "        except AnalysisException:\n",
    "            duplicates_count = 0\n",
    "            print('Table not created yet')\n",
    "        assert duplicates_count == 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src2raw(file_name, validation_column = 'id', partition_column = 'created_at', format = 'json', validations = ['nullity', 'duplication'], validate = True):\n",
    "    sdf = spark.read.option(\"header\", \"true\"\n",
    "                           ).option('path', f's3a://ifood-data-architect-test-source/{file_name}'\n",
    "                               ).format(format\n",
    "                                       ).load(\n",
    "                                            ).withColumn('partition',to_date(col(partition_column).cast('string')))\n",
    "    \n",
    "    if validate: raw_validation(sdf, validation_column, file_name, validations)\n",
    "    \n",
    "    sdf.write.partitionBy('partition').format('parquet').mode('overwrite').saveAsTable(f'raw.{file_name.split(\".\")[0]}')\n",
    "    return sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files: \n",
    "    src2raw(file['name'], format = file['format'], partition_column = file['partition_col'], validation_column = file['validation_col'], validations = file['validations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

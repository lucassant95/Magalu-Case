{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Changing working directory to root to use custom libs\n",
    "os.chdir('/opt/workspace/')\n",
    "\n",
    "# Libs import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import date_format, col, to_date, isnan\n",
    "import pyspark\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('/opt/workspace/')\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from magaluTools.logger import get_logger\n",
    "from magaluTools.dataValidation import *\n",
    "from magaluTools.etl import src2raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Configurations\n",
    "    # Sets Session to use spark master container\n",
    "    # Sets Session to use warehouse directory in /opt/workspace/Warehouse and to infer data Schema\n",
    "    # Sets overide mode to dynamic, so we can append data and overwrite old data based on partition\n",
    "spark = SparkSession.builder.appName('warehouse_setup') \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", '/opt/workspace/Warehouse') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\", 'dynamic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|         id_parceiro|   string|   null|\n",
      "|partner_commissio...|   double|   null|\n",
      "|       partner_bonus|   bigint|   null|\n",
      "|partner_commissio...|   double|   null|\n",
      "|commission_month_...|   string|   null|\n",
      "|# Partition Infor...|         |       |\n",
      "|          # col_name|data_type|comment|\n",
      "|commission_month_...|   string|   null|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe curated.partners_commissions_by_month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating warehouse databases\n",
    "\n",
    "spark.sql('create database if not exists raw')\n",
    "spark.sql('create database if not exists curated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-7a215d74acdc>:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  dump_metadata = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Getting the metadata for correctly dumping the csv files to the warehouse\n",
    "\n",
    "with open('./dump_metadata.yml', 'r') as f:\n",
    "            dump_metadata = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16-Sep-21 20:49:16 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column id_categoria. RESULT: PASSED\n",
      "[16-Sep-21 20:49:17 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column perc_parceiro. RESULT: PASSED\n",
      "[16-Sep-21 20:49:23 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_estado. RESULT: PASSED\n",
      "[16-Sep-21 20:49:23 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_cidade. RESULT: PASSED\n",
      "[16-Sep-21 20:49:38 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column id_cliente. RESULT: PASSED\n",
      "[16-Sep-21 20:49:39 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column nm_cliente. RESULT: PASSED\n",
      "[16-Sep-21 20:49:48 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column id_filial. RESULT: PASSED\n",
      "[16-Sep-21 20:49:49 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column ds_filial. RESULT: PASSED\n",
      "[16-Sep-21 20:49:50 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_cidade. RESULT: PASSED\n",
      "[16-Sep-21 20:49:51 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_pedido. RESULT: PASSED\n",
      "[16-Sep-21 20:49:52 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_produto. RESULT: PASSED\n",
      "[16-Sep-21 20:49:52 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column quantidade. RESULT: PASSED\n",
      "[16-Sep-21 20:49:53 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column vr_unitario. RESULT: PASSED\n",
      "[16-Sep-21 20:49:57 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column id_parceiro. RESULT: PASSED\n",
      "[16-Sep-21 20:49:58 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column nm_parceiro. RESULT: PASSED\n",
      "[16-Sep-21 20:50:00 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_pedido. RESULT: PASSED\n",
      "[16-Sep-21 20:50:01 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column dt_pedido. RESULT: PASSED\n",
      "[16-Sep-21 20:50:02 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_parceiro. RESULT: PASSED\n",
      "[16-Sep-21 20:50:03 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_cliente. RESULT: PASSED\n",
      "[16-Sep-21 20:50:03 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_filial. RESULT: PASSED\n",
      "[16-Sep-21 20:50:04 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column vr_total_pago. RESULT: PASSED\n",
      "[16-Sep-21 20:50:28 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column id_produto. RESULT: PASSED\n",
      "[16-Sep-21 20:50:29 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column ds_produto. RESULT: PASSED\n",
      "[16-Sep-21 20:50:29 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_subcategoria. RESULT: PASSED\n",
      "[16-Sep-21 20:50:32 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column id_subcategoria. RESULT: PASSED\n",
      "[16-Sep-21 20:50:32 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column ds_subcategoria. RESULT: PASSED\n",
      "[16-Sep-21 20:50:33 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Nullity Check for column id_categoria. RESULT: PASSED\n"
     ]
    }
   ],
   "source": [
    "# Iterating over the files in /Data/ Folder to dump the data to the Warehouse\n",
    "\n",
    "for x in Path('./Data').iterdir():\n",
    "    \n",
    "    # Checking if the the current entity is a file or directory\n",
    "    if not x.is_dir(): \n",
    "        \n",
    "        # Seting the name of the table\n",
    "        table_name = x.name.replace('.csv','')\n",
    "        \n",
    "        # Checking if the metadata needed for the etl is in the file, if not, skips the file\n",
    "        if table_name in dump_metadata:\n",
    "            \n",
    "            # Getting the metadata specicallly for the current dump\n",
    "            metadata = dump_metadata[table_name]\n",
    "            \n",
    "            # Using custom function to extract the file data and transform it into parquet files in the warehouse\n",
    "            result_df = src2raw(file_path=f'/opt/workspace/Data/{x.name}',\n",
    "                                table_name=table_name,\n",
    "                                validations=metadata.get('validations'),\n",
    "                                partition_data=metadata.get('partition'),\n",
    "                                csv_delimiter=metadata.get('delimiter'),\n",
    "                                raise_if_validation_failed=True)\n",
    "    \n",
    "        # Printing warning for the user to know the metadata for the csv file was not found\n",
    "        else:\n",
    "            get_logger().warning(f'CSV metadata not found for {x.name}, please create it in the dump_metadata.yml file :D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stops Spark and finishes the job\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libs import\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('/opt/workspace/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing working directory to root to use custom libs\n",
    "os.chdir('/opt/workspace/')\n",
    "\n",
    "#Spark Configurations\n",
    "    # Sets Session to use spark master container\n",
    "    # Sets Session to use warehouse directory in /opt/workspace/Warehouse and to infer data Schema\n",
    "    # Sets overide mode to dynamic, so we can append data and overwrite old data based on partition\n",
    "spark = SparkSession.builder.appName('[CURATED] Orders') \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.sql.streaming.schemaInference\", True) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", '/opt/workspace/Warehouse') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\", 'dynamic') \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+-----------+----------+---------+-------------+----------+----------+-----------+---------+----------+\n",
      "|   order_unique_id|id_pedido|id_parceiro|id_cliente|id_filial|vr_total_pago|id_produto|quantidade|vr_unitario|real_item| partition|\n",
      "+------------------+---------+-----------+----------+---------+-------------+----------+----------+-----------+---------+----------+\n",
      "|498735617376675916|498735617|         16| 144501527|     1367|        64.99|   3766759|         1|      64.99|     true|2021-08-11|\n",
      "|498735617380189016|498735617|         16| 144501527|     1367|        64.99|   3801890|         1|      64.99|     true|2021-08-11|\n",
      "|498735617376675913|498735617|         13| 144501527|     1367|        64.99|   3766759|         1|      64.99|     true|2021-08-11|\n",
      "|498735617380189013|498735617|         13| 144501527|     1367|        64.99|   3801890|         1|      64.99|     true|2021-08-11|\n",
      "+------------------+---------+-----------+----------+---------+-------------+----------+----------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is an ugly query, I admit :(\n",
    "# But in cases like the one presented here, there are two distinct products with the same total value for\n",
    "# the same order. So even comparing the values, we can't determine the source partner that sold the product\n",
    "# To continue this \n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    with a as (\n",
    "        select \n",
    "            id_pedido,\n",
    "            count(*) as c\n",
    "        from raw.pedido\n",
    "        group by id_pedido\n",
    "    ),\n",
    "    b as (\n",
    "        select id_pedido \n",
    "        from a \n",
    "        where c > 1\n",
    "    ),\n",
    "    c as (\n",
    "    select \n",
    "        p.id_pedido || ip.id_produto || p.id_parceiro as order_unique_id,\n",
    "        p.id_pedido,\n",
    "        p.id_parceiro,\n",
    "        p.id_cliente,\n",
    "        p.id_filial,\n",
    "        p.vr_total_pago,\n",
    "        ip.id_produto,\n",
    "        ip.quantidade,\n",
    "        ip.vr_unitario,\n",
    "        case when (vr_unitario * quantidade) = vr_total_pago then true end as real_item,\n",
    "        to_date(p.dt_pedido) as partition\n",
    "    from raw.pedido p\n",
    "        join raw.item_pedido ip on ip.id_pedido = p.id_pedido\n",
    "    where p.id_pedido in (select * from b) and p.id_pedido = 498735617\n",
    "    )\n",
    "    select * from c where real_item is not null\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|id_pedido|\n",
      "+---------+\n",
      "|472106545|\n",
      "|470274295|\n",
      "|498735617|\n",
      "|472752487|\n",
      "|472464102|\n",
      "|487549485|\n",
      "|504600075|\n",
      "|486992785|\n",
      "|488429215|\n",
      "|486914995|\n",
      "|466674577|\n",
      "|505488167|\n",
      "|469276865|\n",
      "|477364955|\n",
      "|498380272|\n",
      "|502719305|\n",
      "|506204825|\n",
      "|474917892|\n",
      "|475735155|\n",
      "|495833187|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is an ugly query, I admit :(\n",
    "# But in cases like the one presented here, there are two distinct products with the same total value for\n",
    "# the same order. So even comparing the values, we can't determine the source partner that sold the product\n",
    "# To continue this case, I'll be desconsidering orders with this behaviour! :D\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW untrackable_orders as\n",
    "    with a as (\n",
    "        select \n",
    "            id_pedido,\n",
    "            count(*) as c\n",
    "        from raw.pedido\n",
    "        group by id_pedido\n",
    "    ),\n",
    "    b as (\n",
    "        select id_pedido \n",
    "        from a \n",
    "        where c > 1\n",
    "    ),\n",
    "    c as (\n",
    "    select \n",
    "       p.id_pedido,\n",
    "       ip.id_produto,\n",
    "       vr_unitario,\n",
    "       quantidade,\n",
    "       vr_total_pago,\n",
    "       case when (vr_unitario * quantidade) = vr_total_pago then true end as related_item\n",
    "    from raw.pedido p\n",
    "        join raw.item_pedido ip on ip.id_pedido = p.id_pedido\n",
    "    where p.id_pedido in (select * from b)\n",
    "    ),\n",
    "    d as (\n",
    "    select \n",
    "        count(distinct (vr_unitario * quantidade)) as calculated_vr_total_pago,\n",
    "        id_pedido,\n",
    "        count(distinct id_produto) as total_dintinct_products\n",
    "    from c \n",
    "    where related_item is not null\n",
    "    group by id_pedido)\n",
    "    select \n",
    "        id_pedido\n",
    "    from d\n",
    "    where calculated_vr_total_pago < total_dintinct_products\n",
    "\"\"\")\n",
    "\n",
    "spark.sql('select * from untrackable_orders').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is an ugly query, I admit :(\n",
    "# But in cases like the one presented here, there are two distinct products with the same total value for\n",
    "# the same order. So even comparing the values, we can't determine the source partner that sold the product\n",
    "# To continue this case, I'll be desconsidering orders with this behaviour! :D\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW normalized_orders as \n",
    "    with a as (\n",
    "        select \n",
    "            id_pedido,\n",
    "            count(*) as c\n",
    "        from raw.pedido\n",
    "        group by id_pedido\n",
    "    ),\n",
    "    b as (\n",
    "        select id_pedido \n",
    "        from a \n",
    "        where c > 1\n",
    "    ),\n",
    "    c as (\n",
    "    select \n",
    "       p.*,\n",
    "       ip.id_produto,\n",
    "       ip.quantidade,\n",
    "       ip.vr_unitario,\n",
    "       case when (vr_unitario * quantidade) = vr_total_pago then true end as related_item\n",
    "    from raw.pedido p\n",
    "        join raw.item_pedido ip on ip.id_pedido = p.id_pedido\n",
    "    where \n",
    "        p.id_pedido in (select * from b)\n",
    "        and p.id_pedido not in (select * from untrackable_orders)\n",
    "    ),\n",
    "    normalized_orders as (\n",
    "    select \n",
    "        id_pedido,\n",
    "        id_parceiro,\n",
    "        id_cliente,\n",
    "        id_filial,\n",
    "        vr_total_pago,\n",
    "        partition,\n",
    "        id_produto,\n",
    "        quantidade,\n",
    "        vr_unitario\n",
    "    from c\n",
    "    where related_item = true)\n",
    "    select\n",
    "        p.id_pedido,\n",
    "        id_parceiro,\n",
    "        id_cliente,\n",
    "        id_filial,\n",
    "        vr_total_pago,\n",
    "        partition,\n",
    "        ip.id_produto,\n",
    "        quantidade,\n",
    "        vr_unitario\n",
    "    from raw.pedido p\n",
    "        join raw.item_pedido ip on ip.id_pedido = p.id_pedido\n",
    "    union all\n",
    "    select * from normalized_orders\n",
    "        \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "    select \n",
    "        o.id_pedido || o.id_produto || o.id_parceiro as order_unique_id,\n",
    "        o.id_pedido,\n",
    "        o.id_parceiro,\n",
    "        o.id_cliente,\n",
    "        o.id_filial,\n",
    "        f.id_cidade,\n",
    "        e.id_estado,\n",
    "        o.id_produto,\n",
    "        c.id_categoria as categoria,\n",
    "        sc.id_subcategoria as subcategoria,\n",
    "        o.partition,\n",
    "        collect_list(map('id_produto', o.id_produto,\n",
    "                        'qtd', o.quantidade,\n",
    "                        'vr_unitario', o.vr_unitario)) as order_partner_items,\n",
    "        round(o.vr_total_pago) as order_partner_value\n",
    "        from normalized_orders o\n",
    "            join raw.produto p on p.id_produto = o.id_produto\n",
    "            join raw.subcategoria sc on sc.id_subcategoria = p.id_subcategoria\n",
    "            join raw.categoria c on c.id_categoria = sc.id_categoria\n",
    "            join raw.filial f on f.id_filial = o.id_filial\n",
    "            join raw.cidade ci on ci.id_cidade = f.id_cidade\n",
    "            join raw.estado e on e.id_estado = ci.id_estado\n",
    "        group by o.id_produto, o.id_pedido, o.id_parceiro, o.id_cliente, o.id_filial, f.id_cidade, e.id_estado, o.partition,\n",
    "        c.id_categoria, sc.id_subcategoria, vr_total_pago\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16-Sep-21 20:55:57 - MainProcess - MainThread] - INFO: [DATA VALIDATION] Unique Key Check for column order_unique_id. RESULT: PASSED\n"
     ]
    }
   ],
   "source": [
    "from magaluTools.dataValidation import *\n",
    "\n",
    "validator = MagaluValidator().setDfToValidate(df).setValidation('order_unique_id', UniqueKeyValidator)\n",
    "validator.validate(True)\n",
    "\n",
    "df.write.partitionBy('partition').format('parquet').mode('overwrite').saveAsTable('curated.normalized_orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stops spark client and finishes the job\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
